{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d60f397-eccb-467b-a314-a229f336d796",
   "metadata": {},
   "source": [
    "# Week 2 · Day 4 — Explainability & Stakeholder-Ready Interpretation\n",
    "\n",
    "## Purpose\n",
    "Fraud models must be transparent and regulator-friendly. Today we:\n",
    "1. Generate **transaction-level explanations** (local explainability) using SHAP.\n",
    "2. Convert explanations into **reason codes** that analysts can review.\n",
    "3. Identify which features most often cause **false positives** (legit transactions flagged as fraud).\n",
    "\n",
    "## Outputs\n",
    "- Notebook: `07_explainability.ipynb`\n",
    "- Review template: Markdown (can be exported to PDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7db5fc1a-b387-4965-9595-027edb7b99c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\caspe\\anaconda\\envs\\genexa_ds\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 1) Imports\n",
    "# =========================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import shap\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec6fd6c-9a69-423d-95ec-2fda53c029fc",
   "metadata": {},
   "source": [
    "## 2) Load Data (Time-Ordered)\n",
    "We keep the same time-based split used in prior days to mimic real-world deployment:\n",
    "- Train = earlier transactions  \n",
    "- Test  = later transactions  \n",
    "\n",
    "This avoids leakage and allows us to test stability across time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d0f7905-ae1e-49e8-b4b8-cadcb7d2c9cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped ID columns: ['transaction_id', 'customer_id', 'device_id', 'ip_address']\n",
      "Train shape: (8832, 22) | Test shape: (2208, 22)\n",
      "Train fraud rate: 0.0768 | Test fraud rate: 0.1409\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# Config\n",
    "# -------------------------\n",
    "DATA_PATH = \"../data/processed/cleaned_transactions.csv\"\n",
    "TIMESTAMP_COL = \"timestamp\"\n",
    "TARGET_COL = \"is_fraud\"\n",
    "\n",
    "# -------------------------\n",
    "# Load\n",
    "# -------------------------\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# Parse timestamp and sort\n",
    "df[TIMESTAMP_COL] = pd.to_datetime(df[TIMESTAMP_COL], errors=\"coerce\")\n",
    "df = df.dropna(subset=[TIMESTAMP_COL]).sort_values(TIMESTAMP_COL).reset_index(drop=True)\n",
    "\n",
    "# -------------------------\n",
    "# Feature groups\n",
    "# -------------------------\n",
    "numeric_features = (\n",
    "    df.select_dtypes(include=[\"number\"])\n",
    "      .columns\n",
    "      .drop([TARGET_COL], errors=\"ignore\")\n",
    "      .tolist()\n",
    ")\n",
    "\n",
    "categorical_features = (\n",
    "    df.select_dtypes(include=[\"object\", \"category\", \"bool\"])\n",
    "      .columns\n",
    "      .drop([TARGET_COL, TIMESTAMP_COL], errors=\"ignore\")\n",
    "      .tolist()\n",
    ")\n",
    "\n",
    "# ✅ NEW: Remove identifier-like columns (high-cardinality, not stakeholder-friendly, can cause leakage)\n",
    "ID_COLS = [\"transaction_id\", \"customer_id\", \"device_id\", \"ip_address\"]\n",
    "categorical_features = [c for c in categorical_features if c not in ID_COLS]\n",
    "\n",
    "# Build X/y\n",
    "X = df[numeric_features + categorical_features]\n",
    "y = df[TARGET_COL].astype(int)\n",
    "\n",
    "# -------------------------\n",
    "# Time split (80/20)\n",
    "# -------------------------\n",
    "split_index = int(len(df) * 0.8)\n",
    "X_train_raw, X_test_raw = X.iloc[:split_index], X.iloc[split_index:]\n",
    "y_train, y_test = y.iloc[:split_index], y.iloc[split_index:]\n",
    "\n",
    "print(\"Dropped ID columns:\", ID_COLS)\n",
    "print(\"Train shape:\", X_train_raw.shape, \"| Test shape:\", X_test_raw.shape)\n",
    "print(\"Train fraud rate:\", round(y_train.mean(), 4), \"| Test fraud rate:\", round(y_test.mean(), 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d913e0c-20a5-4352-8e5d-8b4abbe8f872",
   "metadata": {},
   "source": [
    "## 3) Train the Model to Explain (Reproducible)\r\n",
    "This notebook is separate from Week 2 Day 3, so the trained model objects from that notebook are not available here.\r\n",
    "To ensure reproducibility and transparency, we rebuild and refit the selected model pipeline in this notebook before generating explanations.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b3d31c9-e20d-48cb-8162-b57cd2432641",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "numeric_transformer = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
    "])\n",
    "\n",
    "preprocess = ColumnTransformer([\n",
    "    (\"num\", numeric_transformer, numeric_features),\n",
    "    (\"cat\", categorical_transformer, categorical_features)\n",
    "], remainder=\"drop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed278eb-bed4-4308-b3ef-3d35f03f8893",
   "metadata": {},
   "source": [
    "## Evaluation helper (optional)\n",
    "We use the same metric calculation so results are consistent with Day 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "710be8b5-0ca5-4a57-a930-47ff207ac0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "def eval_at_threshold(y_true, y_proba, threshold=0.5):\n",
    "    y_pred = (y_proba >= threshold).astype(int)\n",
    "    return {\n",
    "        \"threshold\": threshold,\n",
    "        \"precision\": precision_score(y_true, y_pred, zero_division=0),\n",
    "        \"recall\": recall_score(y_true, y_pred, zero_division=0),\n",
    "        \"f1\": f1_score(y_true, y_pred, zero_division=0),\n",
    "        \"roc_auc\": roc_auc_score(y_true, y_proba),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74fcb62-71e6-48fe-a30f-24d8613f69e8",
   "metadata": {},
   "source": [
    "## Train the model we will explain (RF + class weights)\n",
    "We refit the chosen “best” model here so SHAP explanations are generated from an identical pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "909539d3-f5e0-424d-96f4-53e79c91c640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model ready: RandomForestClassifier\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "rf_weighted = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocess),\n",
    "    (\"model\", RandomForestClassifier(\n",
    "        n_estimators=300,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        class_weight=\"balanced_subsample\"\n",
    "    ))\n",
    "])\n",
    "\n",
    "rf_weighted.fit(X_train_raw, y_train)\n",
    "\n",
    "MODEL_PIPELINE = rf_weighted\n",
    "THRESHOLD = 0.5\n",
    "\n",
    "print(\"Model ready:\", MODEL_PIPELINE.named_steps[\"model\"].__class__.__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4daa40-e224-4818-aa0a-47e52b2207d7",
   "metadata": {},
   "source": [
    "## 4) SHAP Setup — Feature Names After Preprocessing\n",
    "SHAP explanations must use the **same feature space** that the model was trained on.\n",
    "Because our pipeline uses OneHotEncoder, we rebuild feature names after preprocessing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3953ffe8-0af1-4677-85d8-97b7e745d45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_names_from_preprocess(preprocess: ColumnTransformer, numeric_features, categorical_features):\n",
    "    num_names = list(numeric_features)\n",
    "    cat_encoder = preprocess.named_transformers_[\"cat\"].named_steps[\"encoder\"]\n",
    "    cat_names = cat_encoder.get_feature_names_out(categorical_features).tolist()\n",
    "    return num_names + cat_names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d502b63-c756-4cff-a772-97a917c39249",
   "metadata": {},
   "source": [
    "## 5) Build Transaction-Level SHAP Explainer (Local Explanations)\n",
    "We explain individual transactions in a regulator-friendly way:\n",
    "- show predicted **fraud probability** (confidence)\n",
    "- show decision (FLAG/ALLOW) based on threshold\n",
    "- show top SHAP contributors as **reason codes**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bc8c0249-be0f-4315-ba70-17789df95d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def make_shap_for_tree_pipeline(trained_pipeline: Pipeline,\n",
    "                               X_background_raw: pd.DataFrame,\n",
    "                               X_explain_raw: pd.DataFrame,\n",
    "                               numeric_features,\n",
    "                               categorical_features):\n",
    "    \"\"\"\n",
    "    Builds SHAP explanations for a tree model inside a preprocessing pipeline.\n",
    "\n",
    "    Key improvements:\n",
    "    - Removes additivity crash risk (check_additivity=False)\n",
    "    - Uses interventional perturbation with a proper background dataset\n",
    "    - Explains in probability space for stakeholder readability\n",
    "    \"\"\"\n",
    "    preprocess = trained_pipeline.named_steps[\"preprocess\"]\n",
    "    model = trained_pipeline.named_steps[\"model\"]\n",
    "\n",
    "    feature_names = get_feature_names_from_preprocess(preprocess, numeric_features, categorical_features)\n",
    "\n",
    "    # Transform background + explain rows into the SAME feature space\n",
    "    X_bg = preprocess.transform(X_background_raw)\n",
    "    X_ex = preprocess.transform(X_explain_raw)\n",
    "\n",
    "    X_bg = X_bg.toarray() if hasattr(X_bg, \"toarray\") else X_bg\n",
    "    X_ex = X_ex.toarray() if hasattr(X_ex, \"toarray\") else X_ex\n",
    "\n",
    "    X_bg_df = pd.DataFrame(X_bg, columns=feature_names)\n",
    "    X_ex_df = pd.DataFrame(X_ex, columns=feature_names)\n",
    "\n",
    "    # ✅ Proper interventional setup + probability output\n",
    "    explainer = shap.TreeExplainer(\n",
    "        model,\n",
    "        data=X_bg_df,\n",
    "        feature_perturbation=\"interventional\",\n",
    "        model_output=\"probability\"\n",
    "    )\n",
    "\n",
    "    shap_values = explainer.shap_values(X_ex_df, check_additivity=False)\n",
    "\n",
    "    # Binary classifier output handling\n",
    "    if isinstance(shap_values, list):\n",
    "        shap_vals = shap_values[1]  # class 1 = fraud\n",
    "    else:\n",
    "        arr = np.array(shap_values)\n",
    "        shap_vals = arr[:, :, 1] if (arr.ndim == 3 and arr.shape[-1] == 2) else arr\n",
    "\n",
    "    # Guard against interaction values (n, p, p)\n",
    "    arr2 = np.array(shap_vals)\n",
    "    if arr2.ndim == 3 and arr2.shape[1] == arr2.shape[2]:\n",
    "        raise ValueError(\"Interaction SHAP detected (n,p,p). Use shap_values, not shap_interaction_values.\")\n",
    "\n",
    "    return explainer, X_ex_df, shap_vals, feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798bcc82-4502-40b7-b3c0-accca4ea7263",
   "metadata": {},
   "source": [
    "## 6) Analyst View — Convert SHAP into Reason Codes\n",
    "This function produces a stakeholder-ready explanation for one transaction:\n",
    "- Fraud probability\n",
    "- Decision (FLAG/ALLOW)\n",
    "- Top factors pushing toward fraud (**reason codes**)\n",
    "- Top factors pushing away from fraud (why it might be legit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "73d5d4eb-69e7-44f3-b4cb-716a7c41e66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def analyst_view(trained_pipeline: Pipeline,\n",
    "                 X_background_raw: pd.DataFrame,\n",
    "                 X_row_raw: pd.DataFrame,\n",
    "                 numeric_features,\n",
    "                 categorical_features,\n",
    "                 threshold=0.5,\n",
    "                 top_k=8):\n",
    "    \"\"\"\n",
    "    Stakeholder-ready transaction explanation:\n",
    "    - fraud probability\n",
    "    - decision using threshold\n",
    "    - top reason codes (positive SHAP)\n",
    "    \"\"\"\n",
    "    proba = float(trained_pipeline.predict_proba(X_row_raw)[:, 1][0])\n",
    "    decision = \"FLAG (fraud suspected)\" if proba >= threshold else \"ALLOW (likely legit)\"\n",
    "\n",
    "    _, X_ex_df, shap_vals, feature_names = make_shap_for_tree_pipeline(\n",
    "        trained_pipeline,\n",
    "        X_background_raw=X_background_raw,\n",
    "        X_explain_raw=X_row_raw,\n",
    "        numeric_features=numeric_features,\n",
    "        categorical_features=categorical_features\n",
    "    )\n",
    "\n",
    "    sv = shap_vals[0]\n",
    "\n",
    "    contrib = pd.DataFrame({\n",
    "        \"feature\": feature_names,\n",
    "        \"feature_value\": X_ex_df.iloc[0].values,\n",
    "        \"shap_value\": sv,\n",
    "        \"abs_shap\": np.abs(sv),\n",
    "    }).sort_values(\"abs_shap\", ascending=False)\n",
    "\n",
    "    top = contrib.head(top_k).copy()\n",
    "    top_pos = top[top[\"shap_value\"] > 0].copy()\n",
    "    top_neg = top[top[\"shap_value\"] < 0].copy()\n",
    "\n",
    "    return {\n",
    "        \"fraud_probability\": proba,\n",
    "        \"threshold\": threshold,\n",
    "        \"decision\": decision,\n",
    "        \"top_contributors_all\": top,\n",
    "        \"top_positive_reasons\": top_pos,\n",
    "        \"top_negative_factors\": top_neg,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc44f3c-09b3-4e28-8d63-784f64516380",
   "metadata": {},
   "source": [
    "## 7) Example: Explain One Flagged Transaction\n",
    "We pick a transaction from the test set and generate a local explanation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "177a9163-5993-4223-aa71-efc1c6ec66b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High confidence indices: [2030, 364, 1258]\n",
      "Borderline indices: [1241, 1645, 315]\n",
      "Decision: FLAG (fraud suspected)\n",
      "Fraud probability: 1.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>feature_value</th>\n",
       "      <th>shap_value</th>\n",
       "      <th>abs_shap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>txn_velocity_24h</td>\n",
       "      <td>1.828476</td>\n",
       "      <td>0.114454</td>\n",
       "      <td>0.114454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>txn_velocity_1h</td>\n",
       "      <td>1.792885</td>\n",
       "      <td>0.100015</td>\n",
       "      <td>0.100015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>risk_score_internal</td>\n",
       "      <td>3.221917</td>\n",
       "      <td>0.087165</td>\n",
       "      <td>0.087165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ip_risk_score</td>\n",
       "      <td>2.277841</td>\n",
       "      <td>0.087065</td>\n",
       "      <td>0.087065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>account_age_days</td>\n",
       "      <td>-0.993465</td>\n",
       "      <td>0.067585</td>\n",
       "      <td>0.067585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>device_trust_score</td>\n",
       "      <td>-1.545503</td>\n",
       "      <td>0.066379</td>\n",
       "      <td>0.066379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>chargeback_history_count</td>\n",
       "      <td>4.079590</td>\n",
       "      <td>0.054024</td>\n",
       "      <td>0.054024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>amount_src_num</td>\n",
       "      <td>0.524115</td>\n",
       "      <td>0.035543</td>\n",
       "      <td>0.035543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8011</th>\n",
       "      <td>location_mismatch_True</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.032933</td>\n",
       "      <td>0.032933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8022</th>\n",
       "      <td>kyc_tier_low</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.031923</td>\n",
       "      <td>0.031923</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       feature  feature_value  shap_value  abs_shap\n",
       "9             txn_velocity_24h       1.828476    0.114454  0.114454\n",
       "8              txn_velocity_1h       1.792885    0.100015  0.100015\n",
       "7          risk_score_internal       3.221917    0.087165  0.087165\n",
       "3                ip_risk_score       2.277841    0.087065  0.087065\n",
       "4             account_age_days      -0.993465    0.067585  0.067585\n",
       "5           device_trust_score      -1.545503    0.066379  0.066379\n",
       "6     chargeback_history_count       4.079590    0.054024  0.054024\n",
       "11              amount_src_num       0.524115    0.035543  0.035543\n",
       "8011    location_mismatch_True       1.000000    0.032933  0.032933\n",
       "8022              kyc_tier_low       1.000000    0.031923  0.031923"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Background sample (reference distribution)\n",
    "X_bg = X_train_raw.sample(min(500, len(X_train_raw)), random_state=42)\n",
    "\n",
    "# Predict test probabilities\n",
    "test_proba = MODEL_PIPELINE.predict_proba(X_test_raw)[:, 1]\n",
    "pred_flag = (test_proba >= THRESHOLD).astype(int)\n",
    "\n",
    "flagged_idx = np.where(pred_flag == 1)[0]\n",
    "if len(flagged_idx) == 0:\n",
    "    print(\"No flagged transactions at this threshold.\")\n",
    "else:\n",
    "    high_conf_idx = flagged_idx[np.argsort(test_proba[flagged_idx])[::-1][:3]]\n",
    "    borderline_idx = flagged_idx[np.argsort(np.abs(test_proba[flagged_idx] - THRESHOLD))[:3]]\n",
    "\n",
    "    print(\"High confidence indices:\", high_conf_idx.tolist())\n",
    "    print(\"Borderline indices:\", borderline_idx.tolist())\n",
    "\n",
    "    idx = int(high_conf_idx[0])\n",
    "    row = X_test_raw.iloc[[idx]]\n",
    "\n",
    "    res = analyst_view(\n",
    "        trained_pipeline=MODEL_PIPELINE,\n",
    "        X_background_raw=X_bg,\n",
    "        X_row_raw=row,\n",
    "        numeric_features=numeric_features,\n",
    "        categorical_features=categorical_features,\n",
    "        threshold=THRESHOLD,\n",
    "        top_k=10\n",
    "    )\n",
    "\n",
    "    print(\"Decision:\", res[\"decision\"])\n",
    "    print(\"Fraud probability:\", round(res[\"fraud_probability\"], 4))\n",
    "    display(res[\"top_positive_reasons\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73350d98-ca55-465b-8191-7983720d502c",
   "metadata": {},
   "source": [
    "### Example flagged transaction — reason codes (local SHAP)\n",
    "\n",
    "For a high-confidence flagged transaction (fraud probability ≈ 1.0), the strongest drivers were:\n",
    "\n",
    "- **High transaction velocity (1h and 24h)**: suggests burst behaviour often associated with fraud rings or account takeover.\n",
    "- **High risk scores (internal + IP risk)**: indicates the transaction shares known fraud signatures and risky network attributes.\n",
    "- **Low account age**: newer accounts tend to have less trust/history and are more frequently used for fraud.\n",
    "- **Low device trust + location mismatch**: suggests the transaction is being initiated from an unusual device/location pattern.\n",
    "- **Low KYC tier**: weaker identity verification increases exposure to fraudulent activity.\n",
    "\n",
    "These reasons can be surfaced as analyst-facing “reason codes” to support manual review and auditability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "32f1eb93-7917-4ece-9ec6-6efa25819fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold = 0.5\n",
      "False positives found: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =========================\n",
    "# False Positive Driver Analysis (SHAP on false positives)\n",
    "# =========================\n",
    "# Goal:\n",
    "#   Identify which features most often push LEGIT transactions (y=0)\n",
    "#   into being flagged as FRAUD (pred=1).\n",
    "#\n",
    "# Output:\n",
    "#   A ranked table of features by mean(|SHAP|) across false positives.\n",
    "#   This supports your reflection question for Day 4.\n",
    "# =========================\n",
    "\n",
    "def false_positive_driver_analysis(\n",
    "    trained_pipeline: Pipeline,\n",
    "    X_train_raw: pd.DataFrame,\n",
    "    X_test_raw: pd.DataFrame,\n",
    "    y_test: pd.Series,\n",
    "    numeric_features,\n",
    "    categorical_features,\n",
    "    threshold=0.5,\n",
    "    top_k=15,\n",
    "    background_size=500\n",
    "):\n",
    "    # 1) Predict fraud probabilities on the test set\n",
    "    y_proba = trained_pipeline.predict_proba(X_test_raw)[:, 1]\n",
    "\n",
    "    # 2) Convert probabilities → predicted labels using chosen threshold\n",
    "    y_pred = (y_proba >= threshold).astype(int)\n",
    "\n",
    "    # 3) False positives = predicted fraud (1) but actually legit (0)\n",
    "    fp_mask = (y_pred == 1) & (y_test.values == 0)\n",
    "    fp_count = int(fp_mask.sum())\n",
    "\n",
    "    print(f\"Threshold = {threshold}\")\n",
    "    print(f\"False positives found: {fp_count}\")\n",
    "\n",
    "    # If no false positives, return early\n",
    "    if fp_count == 0:\n",
    "        return None, fp_mask\n",
    "\n",
    "    # Extract the false positive rows\n",
    "    X_fp = X_test_raw.loc[fp_mask]\n",
    "\n",
    "    # 4) Choose background data for SHAP (reference distribution)\n",
    "    #    Keep it moderate for speed and stability.\n",
    "    X_bg = X_train_raw.sample(min(background_size, len(X_train_raw)), random_state=RANDOM_STATE)\n",
    "\n",
    "    # 5) Compute SHAP values on false positives only\n",
    "    _, X_fp_df, shap_vals_fp, feature_names = make_shap_for_tree_pipeline(\n",
    "        trained_pipeline=trained_pipeline,\n",
    "        X_background_raw=X_bg,\n",
    "        X_explain_raw=X_fp,\n",
    "        numeric_features=numeric_features,\n",
    "        categorical_features=categorical_features\n",
    "    )\n",
    "\n",
    "    # 6) Aggregate: mean absolute SHAP across all FP cases\n",
    "    mean_abs = np.abs(shap_vals_fp).mean(axis=0)\n",
    "\n",
    "    fp_drivers = (\n",
    "        pd.DataFrame({\"feature\": feature_names, \"mean_abs_shap\": mean_abs})\n",
    "        .sort_values(\"mean_abs_shap\", ascending=False)\n",
    "        .head(top_k)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    return fp_drivers, fp_mask\n",
    "\n",
    "\n",
    "# ===== Run FP analysis =====\n",
    "fp_drivers, fp_mask = false_positive_driver_analysis(\n",
    "    trained_pipeline=MODEL_PIPELINE,\n",
    "    X_train_raw=X_train_raw,\n",
    "    X_test_raw=X_test_raw,\n",
    "    y_test=y_test,\n",
    "    numeric_features=numeric_features,\n",
    "    categorical_features=categorical_features,\n",
    "    threshold=THRESHOLD,  # try 0.5 first; later you can test 0.6/0.7\n",
    "    top_k=15\n",
    ")\n",
    "\n",
    "display(fp_drivers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d81a9d1a-1fc8-4a5c-8672-2acfc90c079c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold = 0.4\n",
      "False positives found: 0\n",
      "Threshold = 0.3\n",
      "False positives found: 2\n",
      "\n",
      "Top FP drivers at threshold 0.3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>mean_abs_shap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>account_age_days</td>\n",
       "      <td>0.047683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>amount_src_num</td>\n",
       "      <td>0.036124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>amount_usd_num</td>\n",
       "      <td>0.035883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fee</td>\n",
       "      <td>0.032340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>amount_usd</td>\n",
       "      <td>0.031099</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            feature  mean_abs_shap\n",
       "0  account_age_days       0.047683\n",
       "1    amount_src_num       0.036124\n",
       "2    amount_usd_num       0.035883\n",
       "3               fee       0.032340\n",
       "4        amount_usd       0.031099"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RANDOM_STATE = 42\n",
    "\n",
    "for t in [0.4, 0.3, 0.2, 0.1]:\n",
    "    fp_drivers, fp_mask = false_positive_driver_analysis(\n",
    "        trained_pipeline=MODEL_PIPELINE,\n",
    "        X_train_raw=X_train_raw,\n",
    "        X_test_raw=X_test_raw,\n",
    "        y_test=y_test,\n",
    "        numeric_features=numeric_features,\n",
    "        categorical_features=categorical_features,\n",
    "        threshold=t,\n",
    "        top_k=10\n",
    "    )\n",
    "    if fp_drivers is not None:\n",
    "        print(\"\\nTop FP drivers at threshold\", t)\n",
    "        display(fp_drivers.head(5))\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e064e3-126e-4fb4-bd58-ea532a7f570d",
   "metadata": {},
   "source": [
    "### Reflection — Which features most often lead to false positives and why?\n",
    "\n",
    "At the default operating threshold (0.5), the model produced **0 false positives**, meaning no legitimate transactions were incorrectly flagged in the test set.  \n",
    "To understand false-positive risk under a more aggressive policy, we lowered the threshold. At **threshold = 0.3**, we observed **2 false positives**.\n",
    "\n",
    "The most common drivers of these false positives were:\n",
    "- **account_age_days**\n",
    "- **amount_src_num / amount_usd_num / amount_usd**\n",
    "- **fee**\n",
    "\n",
    "**Interpretation (business context):**\n",
    "These features are strongly associated with fraud patterns (new accounts and high-value transfers). However, they also occur in legitimate scenarios such as new customers making first-time transfers, urgent family support payments, tuition/rent payments, or other high-value remittances.  \n",
    "This explains why lowering the threshold increases the likelihood of false positives: the model becomes more sensitive and begins to treat legitimate “high-risk-looking” behaviour as fraud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b59021f-77ce-4c85-8d8c-715ea4fbc027",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (genexa_ds)",
   "language": "python",
   "name": "genexa_ds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
